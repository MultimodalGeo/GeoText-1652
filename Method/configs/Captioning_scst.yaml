train_file: ['data/finetune/coco_karpathy/coco_karpathy_train.json']
val_file: 'data/finetune/coco_karpathy/coco_karpathy_val.json'
test_file: 'data/finetune/coco_karpathy/coco_karpathy_test.json'

image_root: 'images/coco/'
val_gt_file: 'data/finetune/coco_karpathy/coco_karpathy_val_gt.json'
test_gt_file: 'data/finetune/coco_karpathy/coco_karpathy_test_gt.json'

## Vision Encoder
vision_config: 'configs/config_swinB_384.json'

use_clip_vit: False
#image_res: 384
#patch_size: 16

use_swin: True
image_res: 384
patch_size: 32

## Text Encoder
use_roberta: False
text_config: 'configs/config_bert.json'  # ['configs/config_bert.json', 'configs/config_roberta.json']
text_encoder: 'data/bert-base-uncased'  # ['data/bert-base-uncased', 'data/roberta-base']


## Training
num_dec_layers: 6

batch_size_train: 8
batch_size_test: 32

max_tokens: 40
label_smoothing: 0.1


## generation configs
max_length: 20
min_length: 5
num_beams: 3
prompt: 'a picture of '


# for self-critical sequence training
cider_cached_tokens: 'data/finetune/coco_karpathy/coco-train-words.p'  # path to cached cPickle file used to calculate CIDEr scores
sc_train_sample_n: 5  # number of sampled captions for sc training
sc_baseline_type: 'greedy'  # baseline tyep of REINFORCE algorithm
sc_beam_size: 1  # beam size for scst training


## Other Settings
optimizer: {opt: adamW, lr: 1e-5, weight_decay: 0.01, lr_mult: 2}
schedular: {sched: linear, lr: 1e-5, num_warmup_steps: 0.1, num_training_steps: 44275}
# 8855 * 5 epochs = 44275
eval_steps: 500

